{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping and Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sam-wright-1/Data_Projects/blob/main/Web%20Scraping.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests\n",
    "\n",
    "pip install requests\n",
    "\n",
    "## ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è\n",
    "\n",
    " #### Base of Web communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get() # Read\n",
    "requests.post() # Create\n",
    "requests.patch() # Update (modify)\n",
    "requests.put() # Update (replace)\n",
    "requests.delete() # Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from base64 import b64encode \n",
    "from pprint import pprint \n",
    "import sys\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from six.moves import urllib\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creds\n",
    "BASE_API_URL = \"\"\n",
    "EMAIL = \"\"\n",
    "API_KEY = \"\"\n",
    "\n",
    "#The cell creates a connection with our database and pulls the last order that was put into the database.\n",
    "\n",
    "#Make sure the input the login creds\n",
    "params = urllib.parse.quote_plus('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                                 'SERVER=;'\n",
    "                                 'DATABASE=;'\n",
    "                                 'Trusted_Connection=;'\n",
    "                                 'UID=;'\n",
    "                                 'PWD='\n",
    "                                )\n",
    "\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "\n",
    "engine.connect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the authentication step\n",
    "basic_auth = b64encode(f'{GORGIAS_EMAIL}:{GORGIAS_API_KEY}'.encode('utf-8')).decode('utf-8')\n",
    "newheaders = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Basic {basic_auth}'\n",
    "}\n",
    "\n",
    "#this grabs the total pages needed\n",
    "Pages = requests.get(f'{GORGIAS_BASE_API_URL}/tickets/?per_page=100',\n",
    "            headers= newheaders)\n",
    "Request_Pages = pd.json_normalize(Pages.json())\n",
    "Num_Pages = Request_Pages['meta.nb_pages']\n",
    "num = int(Num_Pages)\n",
    "# num can be used to loop through to get all of the results\n",
    "\n",
    "Request_Answer = requests.get(f'{GORGIAS_BASE_API_URL}/tickets/?view_id=12734&per_page=100',\n",
    "          headers= newheaders)\n",
    "\n",
    "Ticket_Dataframe = pd.json_normalize(Request_Answer.json())\n",
    "Ticket_Data_Dataframe = Ticket_Dataframe['data']\n",
    "\n",
    "AllTickets = pd.json_normalize(Ticket_Data_Dataframe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTickets_Database.to_sql(name='',con=engine, index=False, if_exists='replace', method = 'multi',chunksize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "pip install bs4\n",
    "\n",
    "## üëçüëçüëçüëçüëçüëçüëçüëç\n",
    "\n",
    "\n",
    "###### Find data by HTML Tags\n",
    "\n",
    "###### Ways to parse through data\n",
    " - html: standard\n",
    " - lxml: goes fast\n",
    " - html5lib: does it like a web page would but slow\n",
    " - xml: supports xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag is the BeatuifulSoup object\n",
    "\n",
    "print(page.status_code) - lets you know if the request was successful\n",
    "print(page.headers) \n",
    "\n",
    "soup.get_text() # gets all of the text from a page\n",
    "soup.find('h2', class_='title')\n",
    "soup.find('div', class_='company')\n",
    "soup.findall(\"a\", attrs={'class':''}) # finding all links (a-hrefs) in a certain class\n",
    "\n",
    "tag.contents # gets the values inside of the html tags after you find the one you are looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: grabbing stock prices from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOG - 2,031.36\n",
      "DAL - 48.25\n",
      "RACE - 194.83\n",
      "TSLA - 682.22\n",
      "ADSK - 284.03\n",
      "AMZN - 3,057.16\n",
      "DOMO - 61.65\n",
      "OKTA - 256.28\n",
      "AAPL - 120.99\n",
      "NKE - 135.54\n",
      "ADBE - 459.16\n",
      "CRM - 231.08\n",
      "LOGI - 105.83\n",
      "PS - 20.70\n",
      "IBM - 122.47\n",
      "BABA - 240.18\n"
     ]
    }
   ],
   "source": [
    "# Stocks you want to check\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "stock_list = {'GOOG', 'TSLA', 'ADBE', 'RACE', 'DOMO','NKE', 'DAL', 'CRM', 'PS', 'ADSK', 'AAPL', 'AMZN', 'BABA', 'IBM', 'LOGI', 'OKTA'}\n",
    "\n",
    "for urls in stock_list:\n",
    "\n",
    "    URL = 'https://finance.yahoo.com/quote/'+ urls + '?p= '+ urls +'&.tsrc=fin-srch'\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tag = soup.find('span', class_='Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)') # Creates a bs4 element tag.\n",
    "\n",
    "    value = tag.contents[0] # list\n",
    "    print(urls,'-',value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Price and Description of Amazon Product\n",
    "\n",
    "#### Not my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib\n",
    "from pprint import pprint\n",
    "\n",
    "headers = {\n",
    "    \"User-agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0'}\n",
    "\n",
    "URL = 'https://www.amazon.de/dp/B07XVWXW1Q/ref=sr_1_10?keywords=laptop&qid=1581888312&sr=8-10'\n",
    "def amazon_de():\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"productTitle\").get_text()\n",
    "    price = soup.find(id=\"priceblock_ourprice\").get_text()\n",
    "    sep = ','\n",
    "    con_price = price.split(sep, 1)[0]\n",
    "    converted_price = int(con_price.replace('.', ''))\n",
    "\n",
    "    # price\n",
    "    print(title.strip())\n",
    "    print(converted_price)\n",
    "    \n",
    "    return converted_price, title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "price, title = amazon_de()\n",
    "\n",
    "# Could do\n",
    "if price < 50:\n",
    "    email = True\n",
    "else:\n",
    "    email = False\n",
    "    \n",
    "print(email)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if email = True:\n",
    "\n",
    "    email = \n",
    "    password = \n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login(email, password)\n",
    "    subject = 'Price fell down!'\n",
    "    body = 'Check the link:\\\n",
    "    https://www.amazon.de/gp/product/B0756CYWWD/ref=as_li_tl?ie=UTF8&tag=idk01e-21&camp=1638&creative=6742&linkCode=as2&creativeASIN=B0756CYWWD&linkId=18730d371b945bad11e9ea58ab9d8b32'\n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    server.sendmail(\n",
    "    'Price check',\n",
    "    'email-address',\n",
    "    msg\n",
    "    )\n",
    "    print('Hey Email has been sent!')\n",
    "    server.quit()\n",
    "else:\n",
    "    print('not below 50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Amazon Example\n",
    "\n",
    "#### Not my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Title = Sony Playstation 4 Console - 1TB Slim Edition Jet Black - PS4 with 1 DualShock 4 Wireless Controller - Family Holiday Gaming Bundle - iPuzzle Red Reindeer Dust Cover + 3 Feet HDMI Cable\n",
      "Product Price = $499.00\n",
      "Product Rating = 4.4 out of 5 stars\n",
      "Number of Product Reviews = 36 ratings\n",
      "Availability = Only 8 left in stock - order soon.\n",
      "Product Title = 2020 Playstation 4 PS4 Slim 1TB Console Holiday Bundle, Light & Slim PS4 System, 1TB Hard Drive Ghost Manta Charging Station Dock Bundle (PS4 Slim)\n",
      "Product Price = $499.99\n",
      "Product Rating = 4.5 out of 5 stars\n",
      "Number of Product Reviews = 60 ratings\n",
      "Availability = In Stock.\n",
      "Product Title = PlayStation 4 Slim 1TB Console\n",
      "Product Price = \n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 11,414 ratings\n",
      "Availability = Not Available\n",
      "Product Title = PlayStation 4 Console - 1TB Slim Edition\n",
      "Product Price = $371.50\n",
      "Product Rating = 4.6 out of 5 stars\n",
      "Number of Product Reviews = 4,084 ratings\n",
      "Availability = Only 19 left in stock - order soon.\n",
      "Product Title = SONY PlayStation 4 Slim 1TB Console, Light & Slim PS4 System, 1TB Hard Drive, All the Greatest Games, TV, Music & More\n",
      "Product Price = $371.80\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 624 ratings\n",
      "Availability = In Stock.\n",
      "Product Title = PlayStation 4 Slim 1TB Console - Only On PlayStation Bundle\n",
      "Product Price = $464.00\n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 7,916 ratings\n",
      "Availability = Only 2 left in stock - order soon.\n",
      "Product Title = PlayStation 4 Slim 1TB Limited Edition Console - Days of Play Bundle [Discontinued]\n",
      "Product Price = $698.00\n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 689 ratings\n",
      "Availability = In stock.\n",
      "Product Title = PlayStation 4 Slim 1TB Limited Edition Console - Days of Play Bundle\n",
      "Product Price = $486.35\n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 543 ratings\n",
      "Availability = Only 2 left in stock - order soon.\n",
      "Product Title = PlayStation 4 Pro 1TB Limited Edition Console - Marvel's Spider-Man Bundle [Discontinued]\n",
      "Product Price = $795.90\n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 514 ratings\n",
      "Availability = In stock.\n",
      "Product Title = PlayStation 4 Slim 1TB Console System with FIFA 18 Ultimate Team Bundle\n",
      "Product Price = $499.99\n",
      "Product Rating = 4.1 out of 5 stars\n",
      "Number of Product Reviews = 5 ratings\n",
      "Availability = Only 1 left in stock - order soon.\n",
      "Product Title = Playstation 4 Slim 1TB Console with Two DS4 Wireless Controller and Mytrix DS4 Fast Charging Dock\n",
      "Product Price = $529.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 43 ratings\n",
      "Availability = Only 5 left in stock - order soon.\n",
      "Product Title = Mytrix Playstation 4 Slim 2TB Console with DualShock 4 Wireless Controller Bundle, Playstation Enhanced\n",
      "Product Price = $519.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 284 ratings\n",
      "Availability = Only 18 left in stock - order soon.\n",
      "Product Title = PlayStation 4 Pro 1TB Console\n",
      "Product Price = \n",
      "Product Rating = 4.8 out of 5 stars\n",
      "Number of Product Reviews = 10,357 ratings\n",
      "Availability = Not Available\n",
      "Product Title = PlayStation 4 Slim 1TB Console - Black (Renewed)\n",
      "Product Price = $384.52\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 880 ratings\n",
      "Availability = In stock.\n",
      "Product Title = Sony Playstation 4 Console - 1TB Slim Edition Jet Black - PS4 with 1 DualShock 4 Wireless Controller - Family Holiday Gaming Bundle - iPuzzle Red Reindeer Dust Cover + 6 Feet HDMI Cable\n",
      "Product Price = $509.00\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 36 ratings\n",
      "Availability = Only 7 left in stock - order soon.\n",
      "Product Title = Mytrix Playstation 4 Slim 2TB Console with DualShock 4 Wireless Controller and HDMI Bundle, Playstation Enhanced\n",
      "Product Price = $559.99\n",
      "Product Rating = Previous page of related Sponsored Products\n",
      "Number of Product Reviews = \n",
      "Availability = In Stock.\n",
      "Product Title = Playstation 4 Slim 1TB Console with Black and Gold Wireless Controller and Mytrix DS4 Fast Charging Dock\n",
      "Product Price = $529.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 13 ratings\n",
      "Availability = Only 8 left in stock - order soon.\n",
      "Product Title = PlayStation 4 Slim 500GB Console [Discontinued]\n",
      "Product Price = $375.99\n",
      "Product Rating = 4.6 out of 5 stars\n",
      "Number of Product Reviews = 1,327 ratings\n",
      "Availability = Only 10 left in stock - order soon.\n",
      "Product Title = Sony PlayStation 4 Pro 1TB Console - Black (PS4 Pro)\n",
      "Product Price = $661.79\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 2,989 ratings\n",
      "Availability = In Stock.\n",
      "Product Title = PlayStation 4 Slim 1TB Console - Marvel's Spider-Man Bundle [Discontinued]\n",
      "Product Price = $390.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 2,120 ratings\n",
      "Availability = Only 10 left in stock - order soon.\n",
      "Product Title = 2020 Oculus Quest 2 All-in-One VR Headset, 64GB SSD, Glasses Compitble, 3D Audio, Mytrix Carrying Case, Earphone, Oculus Link Cable (10 Ft), Grip Cover, Lens Cover\n",
      "Product Price = $479.99\n",
      "Product Rating = Previous page of related Sponsored Products\n",
      "Number of Product Reviews = \n",
      "Availability = In Stock.\n",
      "Product Title = Sony PlayStation 4 Console, Renewed, Black\n",
      "Product Price = $399.95\n",
      "Product Rating = 4.4 out of 5 stars\n",
      "Number of Product Reviews = 182 ratings\n",
      "Availability = Only 1 left in stock - order soon.\n",
      "Product Title = Sony Playstation 4 Console - 1TB Slim Edition Jet Black - PS4 with 1 DualShock 4 Wireless Controller - Family Holiday Gaming - iPuzzle 4 Colors Silicone Cover Skin Protector + 3 Feet HDMI Cable\n",
      "Product Price = $499.00\n",
      "Product Rating = 4.6 out of 5 stars\n",
      "Number of Product Reviews = 12 ratings\n",
      "Availability = Only 8 left in stock - order soon.\n",
      "Product Title = Holiday Ultimate Bundle Playstation 4 1TB Slim w/bonus Marvel's Spider-Man: Game of The Year Edition\n",
      "Product Price = $488.00\n",
      "Product Rating = 4.6 out of 5 stars\n",
      "Number of Product Reviews = 752 ratings\n",
      "Availability = In stock.\n",
      "Product Title = PlayStation 4 Pro Console with A Dual-Shock Controller and HDMI Cable, Stream 4K Video Capable for up to 4 Players- Jet Black\n",
      "Product Price = $657.99\n",
      "Product Rating = 4.3 out of 5 stars\n",
      "Number of Product Reviews = 62 ratings\n",
      "Availability = Only 1 left in stock - order soon.\n",
      "Product Title = Playstation 4 Slim 1TB Console with Black and Gold Wireless Controller and Mytrix DS4 Fast Charging Dock\n",
      "Product Price = $529.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 13 ratings\n",
      "Availability = Only 8 left in stock - order soon.\n",
      "Product Title = Playstation 4 Slim 1TB Console with Two DS4 Wireless Controller and Mytrix DS4 Fast Charging Dock\n",
      "Product Price = $529.99\n",
      "Product Rating = 4.7 out of 5 stars\n",
      "Number of Product Reviews = 43 ratings\n",
      "Availability = Only 5 left in stock - order soon.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "     \n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    " \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.string\n",
    " \n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    " \n",
    "        # # Printing types of values for efficient understanding\n",
    "        # print(type(title))\n",
    "        # print(type(title_value))\n",
    "        # print(type(title_string))\n",
    "        # print()\n",
    " \n",
    "    except AttributeError:\n",
    "        title_string = \"\"   \n",
    " \n",
    "    return title_string\n",
    " \n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    " \n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    " \n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    " \n",
    "        except:     \n",
    "            price = \"\"  \n",
    " \n",
    "    return price\n",
    " \n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    " \n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "         \n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\" \n",
    " \n",
    "    return rating\n",
    " \n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "        review_count = \"\"   \n",
    " \n",
    "    return review_count\n",
    " \n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\n",
    " \n",
    "    return available    \n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # Headers for request\n",
    "    HEADERS = ({'User-Agent':\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "                'Accept-Language': 'en-US'})\n",
    " \n",
    "    # The webpage URL\n",
    "    URL = \"https://www.amazon.com/s?k=playstation+4&ref=nb_sb_noss_2\"\n",
    "     \n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    " \n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    " \n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    " \n",
    "    # Store the links\n",
    "    links_list = []\n",
    " \n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    " \n",
    " \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    " \n",
    "        new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    " \n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "         \n",
    "        # Function calls to display all necessary product information\n",
    "        print(\"Product Title =\", get_title(new_soup))\n",
    "        print(\"Product Price =\", get_price(new_soup))\n",
    "        print(\"Product Rating =\", get_rating(new_soup))\n",
    "        print(\"Number of Product Reviews =\", get_review_count(new_soup))\n",
    "        print(\"Availability =\", get_availability(new_soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium \n",
    "\n",
    "## üî•üî•üî•üî•üî•üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get() # Opens up the page\n",
    "title # prints webpage title\n",
    "quit() # Quit browser \n",
    "close() # Close tab\n",
    "click()\n",
    "send_keys()\n",
    "find_element()\n",
    "back()\n",
    "forward()\n",
    "clear()\n",
    "\n",
    "https://selenium-python.readthedocs.io/locating-elements.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    jobs = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"main\"))\n",
    "    )\n",
    "except:\n",
    "    print('not working')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Jobs off Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XPath is a query language for selecting nodes from an XML document. You can use it with HTML and its really handy.\n",
    "#  Basically it just shows the path to where the element is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Webdriver performs the actions and syncs up with the page\n",
    "from selenium import webdriver # Have the ability to open up a page and access it\n",
    "from selenium.webdriver.common.keys import Keys # access to enter key, escape key, etc.\n",
    "from selenium.webdriver.common.by import By # the thing to search \n",
    "from selenium.webdriver.support.ui import WebDriverWait # Wait until something renders\n",
    "from selenium.webdriver.support import expected_conditions as EC # wait until a condition is met\n",
    "from selenium.webdriver.common.action_chains import ActionChains # perform more advanced tasks (double clicks, drag and drop, etc.)\n",
    "\n",
    "import time\n",
    "\n",
    "# Set up Selenium\n",
    "path_to_chromedriver = 'C:\\Program Files (x86)\\chromedriver.exe' # this part needs the chrome driver installed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver) # This is where the driver lives\n",
    "\n",
    "\n",
    "# ONE WAY TO DO IT\n",
    "search_term = 'Data Science Intern'\n",
    "search = search_term.replace(' ', '+')\n",
    "\n",
    "URL_last_three = 'https://www.google.com/search?q='+ search +'&rlz=1C1CHBF_enUS908US908&oq=goiogle+jovs&aqs=chrome..69i57j0i13i457j0i13l8.1319j0j7&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiju-3NoPXuAhVQpZ4KHTjRDgUQutcGKAF6BAgdEAk&sxsrf=ALeKk01FkBWPIp-F8Rs4nOmTOe-XTXosyw:1613713865670#fpstate=tldetail&htivrt=jobs&htichips=date_posted:3days&htischips=date_posted;3days&htidocid=Ob4dWFbxVOjFasLVAAAAAA%3D%3D'\n",
    "URL_all = 'https://www.google.com/search?q='+ search +'&rlz=1C1CHBF_enUS908US908&oq=goiogle+jovs&aqs=chrome..69i57j0i13i457j0i13l8.1319j0j7&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiju-3NoPXuAhVQpZ4KHTjRDgUQutcGKAF6BAgdEAk&sxsrf=ALeKk01FkBWPIp-F8Rs4nOmTOe-XTXosyw:1613713865670#fpstate=tldetail&htivrt=jobs&htidocid=TDq3YZR6oHJft8dBAAAAAA%3D%3D'\n",
    "\n",
    "# Use selenium\n",
    "\n",
    "browser.get(URL_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver # Have the ability to open up a page and access it\n",
    "from selenium.webdriver.common.keys import Keys # access to enter key, escape key, etc.\n",
    "from selenium.webdriver.common.by import By # the thing to search \n",
    "from selenium.webdriver.support.ui import WebDriverWait # Wait until something renders\n",
    "from selenium.webdriver.support import expected_conditions as EC # wait until a condition is met\n",
    "from selenium.webdriver.common.action_chains import ActionChains # perform more advanced tasks (double clicks, drag and drop, etc.)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Search jobs\n",
    "search_term = 'BI Analyst'\n",
    "\n",
    "# How many job postings do you want?\n",
    "job_postings = 15\n",
    "\n",
    "# Set up Selenium\n",
    "path_to_chromedriver = 'C:\\Program Files (x86)\\chromedriver.exe' # this part needs the chrome driver installed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver) # This is where the driver lives\n",
    "\n",
    "\n",
    "# Use selenium\n",
    "browser.get('https://www.google.com/search?q=job&rlz=1C1CHBF_enUS908US908&oq=google+jobs&aqs=chrome.0.69i59j0l2j0i433l2j69i60l3.1965j0j4&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiM-Liuiv_uAhU5GDQIHZFoCvoQutcGKAF6BAgdEAk&sxsrf=ALeKk02jKAPaNxuq0NajGP0eUDp3I8To1g:1614051491600#fpstate=tldetail&htivrt=jobs&htidocid=6ro2iZ7gqMVEcaUzAAAAAA%3D%3D') # Opens up the page\n",
    "\n",
    "\n",
    "# This part gets us the jobs from the search term and the ones that are the newest\n",
    "search = browser.find_element_by_id(\"hs-qsb\")\n",
    "search.clear()\n",
    "search.send_keys(search_term)\n",
    "search.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    search = browser.find_element_by_xpath('//*[@id=\"choice_box_root\"]/div[1]/div[1]/span[3]')\n",
    "    search.click()\n",
    "    \n",
    "    time.sleep(2) \n",
    "    \n",
    "    search = browser.find_element_by_xpath('//div[@data-name = \"3days\"]')\n",
    "    search.click()\n",
    "    \n",
    "except:\n",
    "    search = browser.find_element_by_xpath('//*[@id=\"choice_box_root\"]/div[1]/div[1]/span[4]')\n",
    "    search.click()\n",
    "    \n",
    "    time.sleep(2) \n",
    "    \n",
    "    search = browser.find_element_by_xpath('//div[@data-name = \"3days\"]')\n",
    "    search.click()\n",
    "    \n",
    "time.sleep(2) \n",
    "\n",
    "\n",
    "# userNameElement= WebDriverWait(browser, 10).until(\n",
    "# EC.presence_of_element_located((By.XPATH, '//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[1]/div/div[1]/div[2]'))\n",
    "# )\n",
    "# userNameElement.click()\n",
    "\n",
    "\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[1]/div/div[1]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[2]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[3]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[4]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[5]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[6]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[7]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[8]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[9]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[10]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[2]/div/ul/li[1]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[2]/div/ul/li[8]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[3]/div/ul/li[1]/div/div[2]/div[2]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Better way to do it\n",
    "\n",
    "\n",
    "x = 30 # Amount of job postings I want \n",
    "y = int(x / 10) # pull 10 accourding to html\n",
    "z = x % 10 # remainder\n",
    "\n",
    "# This is the way I scroll\n",
    "for i in range(1,y+1):\n",
    "    scroll = ActionChains(browser)\n",
    "    scroll.move_to_element(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(i) + ']/div/ul/li[9]/div/div[2]/div[2]'))\n",
    "    scroll.click()\n",
    "    scroll.pause(2)\n",
    "    scroll.perform()\n",
    "    \n",
    "# Finds loads\n",
    "better = browser.find_elements_by_class_name('gws-plugins-horizon-jobs__tl-lif')\n",
    "for elements in better:\n",
    "    actions = ActionChains(browser)\n",
    "    actions.move_to_element(elements)\n",
    "    actions.click()\n",
    "    actions.pause(1)\n",
    "    actions.perform()\n",
    "\n",
    "\"\"\"    \n",
    "    \n",
    "x = job_postings # Amount of job postings I want \n",
    "\n",
    "y = int(x / 10) # pull 10 accourding to html\n",
    "z = x % 10 # remainder\n",
    "\n",
    "# This is the way I scroll to populate further pages\n",
    "for i in range(1,y+1):\n",
    "    scroll = ActionChains(browser)\n",
    "    scroll.move_to_element(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(i) + ']/div/ul/li[9]/div/div[2]/div[2]'))\n",
    "    scroll.click()\n",
    "    scroll.pause(2)\n",
    "    scroll.perform()\n",
    "\n",
    "# All of the elements that we need to access\n",
    "tree_items = []\n",
    "\n",
    "for m in range(1,y + 1):\n",
    "    for n in range(1,11):\n",
    "        if m == 1 and n == 1:\n",
    "            l = 1\n",
    "        else: \n",
    "            l = 2\n",
    "        tree_items.append(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(m) + ']/div/ul/li[' + str(n) + ']/div/div[' + str(l) + ']/div[2]/div/div'))\n",
    "    \n",
    "remainder_items = []\n",
    "\n",
    "for k in range(1,z+1):\n",
    "    if y == 0 and k == 1:\n",
    "        l = 1\n",
    "    else: \n",
    "        l = 2\n",
    "    remainder_items.append(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(y+1) + ']/div/ul/li[' + str(k) + ']/div/div[' + str(l) + ']/div[2]/div/div'))\n",
    "\n",
    "\n",
    "    \n",
    "total_list = []\n",
    "job_list = []\n",
    "\n",
    "for elements in tree_items:\n",
    "        first = ActionChains(browser)\n",
    "        first.move_to_element(elements)\n",
    "        first.click()\n",
    "        first.pause(2)\n",
    "        first.perform()\n",
    "        \n",
    "        job_list = []\n",
    "        \n",
    "        try:\n",
    "            company_name = browser.find_elements_by_xpath('//*[@id=\"gws-plugins-horizon-jobs__job_details_page\"]/div/div[1]/div/div[2]/div[2]/div[1]')\n",
    "            for i in company_name:\n",
    "                if i.is_displayed():\n",
    "                    company_name_final = i.text\n",
    "        except:\n",
    "            company_name_final = 'no company name'\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            title = browser.find_elements_by_class_name('KLsYvd')\n",
    "            for i in title:\n",
    "                if i.is_displayed():\n",
    "                    title_final = i.text\n",
    "                \n",
    "        except:\n",
    "            title_final = 'no title'\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # review = browser.find_element_by_xpath('//*[@id=\"fc_fDw1YPeFLP-V0PEPxZqi0A06\"]/div/a[1]/div/div[2]').get_attribute('textContent')\n",
    "            description = browser.find_elements_by_class_name('YgLbBe')\n",
    "            for i in description:\n",
    "                if i.is_displayed():\n",
    "                    description_final = i.text\n",
    "                \n",
    "        except:\n",
    "            description_final = 'no description'\n",
    "                \n",
    "                        \n",
    "        try:\n",
    "            \n",
    "            reviews = browser.find_elements_by_class_name('o1h2dc')\n",
    "            for i in reviews:\n",
    "                if i.is_displayed():\n",
    "                    reviews_final = i.text\n",
    "\n",
    "        except:\n",
    "            \n",
    "            reviews_final = 'no reviews'\n",
    "            \n",
    "        \n",
    "        job_list.append(company_name_final)\n",
    "        \n",
    "        job_list.append(title_final)\n",
    "        \n",
    "        job_list.append(reviews_final)\n",
    "        \n",
    "        job_list.append(description_final)\n",
    "        \n",
    "        # Append to the total list\n",
    "        total_list.append(job_list)\n",
    "\n",
    "        \n",
    "for elements in remainder_items:\n",
    "        second = ActionChains(browser)\n",
    "        second.move_to_element(elements)\n",
    "        second.click()\n",
    "        second.pause(2)\n",
    "        second.perform()\n",
    "        \n",
    "        job_list = []\n",
    "        \n",
    "        try:\n",
    "            company_name = browser.find_elements_by_xpath('//*[@id=\"gws-plugins-horizon-jobs__job_details_page\"]/div/div[1]/div/div[2]/div[2]/div[1]')\n",
    "            for i in company_name:\n",
    "                if i.is_displayed():\n",
    "                    company_name_final = i.text\n",
    "        except:\n",
    "            company_name_final = 'no company name'\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            title = browser.find_elements_by_class_name('KLsYvd')\n",
    "            for i in title:\n",
    "                if i.is_displayed():\n",
    "                    title_final = i.text\n",
    "                \n",
    "        except:\n",
    "            title_final = 'no title'\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # review = browser.find_element_by_xpath('//*[@id=\"fc_fDw1YPeFLP-V0PEPxZqi0A06\"]/div/a[1]/div/div[2]').get_attribute('textContent')\n",
    "            description = browser.find_elements_by_class_name('YgLbBe')\n",
    "            for i in description:\n",
    "                if i.is_displayed():\n",
    "                    description_final = i.text\n",
    "                \n",
    "        except:\n",
    "            description_final = 'no description'\n",
    "                \n",
    "                        \n",
    "        try:\n",
    "            \n",
    "            reviews = browser.find_elements_by_class_name('o1h2dc')\n",
    "            for i in reviews:\n",
    "                if i.is_displayed():\n",
    "                    reviews_final = i.text\n",
    "\n",
    "        except:\n",
    "            \n",
    "            reviews_final = 'no reviews'\n",
    "            \n",
    "        \n",
    "        job_list.append(company_name_final)\n",
    "        \n",
    "        job_list.append(title_final)\n",
    "        \n",
    "        job_list.append(reviews_final)\n",
    "        \n",
    "        job_list.append(description_final)\n",
    "        \n",
    "        # Append to the total list\n",
    "        total_list.append(job_list)\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Utah Valley University',\n",
       "  'Student - Data Analysis and Business Intelligence',\n",
       "  ' 4.3  -  14 reviews',\n",
       "  'Under the direction of the Developmental Mathematics Department Chair, performs data analysis and prepares reports.\\n\\nRequired Qualifications\\n\\nMust be a current UVU student.\\n\\nKnowledge, Skills and Abilities\\n‚Ä¢ Knowledge of reporting and business intelligence tools, such as Tableau.\\n‚Ä¢ Knowledge of statistical data collection, analysis, tracking, and reporting systems, methods, and techniques.\\n‚Ä¢ Ability to program and/or learn programming languages such as SQL.\\n‚Ä¢ Knowledge of databases.\\n‚Ä¢ Skill in the use of personal computers and related software applications.\\n‚Ä¢ Skill in collecting, analyzing, verifying, and manipulating research data.\\n‚Ä¢ Ability to learn new software tools.\\n‚Ä¢ Strong problem-solving ability.\\n‚Ä¢ Detail-oriented.\\n\\nPreferred Qualifications\\n\\nPhysical Requirements\\n\\nLess than 28 hours per week.\\n\\nEEO Statement\\n\\nEmployment decisions are made on the basis of an applicant‚Äôs qualifications and ability to perform the job without regard to race, color, religion, national origin, sex...\\nREAD MORE'],\n",
       " ['SAP',\n",
       "  'Sales Operations BI Analyst',\n",
       "  ' 4.4  -  3,353 reviews',\n",
       "  '‚Ä¢ If you have a Bachelors degree and 2-4 years of experience in data analytics (including SQL, a data visualization platform, and one or more programming languages), Qualtrics could be the place to take your career to the next level.\\n‚Ä¢ The ChallengeThe Qualtrics Sales Operations team is looking for someone to join us and have a major impact on the companys performance.\\n‚Ä¢ We are looking for a Data Analyst/Scientist who wants to make an impact on a fast-growing business, can conduct in depth analytics and reporting, and can make strategic recommendations for the company.\\n‚Ä¢ If you are interested in applying for employment with SAP and are in need of accommodation or special assistance to navigate our website or to complete your application, please contact us: EMEA - Americas - Americas - APJ -\\n‚Ä¢ You can work cross-functionally, are a self-starter, can multitask, and manage your time effectively.\\n‚Ä¢ Youll know youre doing a great job when our sales team has easy to understand insights to...\\nREAD MORE'],\n",
       " ['Spectrum',\n",
       "  '2021 Summer Internship: BI Analyst I (TO-89AFD)',\n",
       "  ' 3.8  -  223 reviews',\n",
       "  \"**AT A GLANCE**You're a motivated rising junior or above student graduating in May 2021 with a 3.0 GPA or higher seeking a degree in one of the following areas listed below from an accredited college or university:Business DevelopmentThis is a learning-intensive program designed to give you essential business insights and hands-on experience in your field of choice. It's a full-time, 10-week commitment from JUNE 2, 2021 through AUGUST 6, 2021.Benefits include professional development sessions, networking opportunities, and mentorship.**THE SPECTRUM INTERNSHIP EXPERIENCE** You have clear aspirations and are seeking a summer internship program that will help you meet them. Find it at Spectrum, named one of the Top 100 Internship Programs in the United States by WayUp. Our internships are designed to provide: Opportunities to gain new skills and elevate the ones you already have, all in a robust and forward-thinking business setting.First-rate, hands-on experience in the...\\nREAD MORE\"],\n",
       " ['Stride Health',\n",
       "  'Business Intelligence Analyst (Salt Lake City, UT)',\n",
       "  ' 3.0  -  148 reviews',\n",
       "  'About the RoleWe‚Äôre looking for a driven and ambitious business intelligence analyst to empower our Product and Sales teams with data-driven insights that drive business results. You will lead the analysis and reporting on key organizational and business metrics, build and maintain our primary dashboards, and develop business cases to support new initiatives.ResponsibilitiesAnalyze, track and report on key organizational and business metrics Maintain and develop dashboards, tools, and analysis to empower the Product and Sales teams with data-driven insightsCombine external market data with internal sales and analytics data to inform our business strategyDevelop quantitative and data-based business cases to support new or existing initiatives, extrapolating trends to forecast future growth potentialQualifications4+ years of relevant work experience or a Master‚Äôs degree in a quantitative field with 2+ years of experienceAgility and comfort with data to turn ambiguous situations into...\\nREAD MORE'],\n",
       " ['PURPLE INNOVATION LLC',\n",
       "  'Business Intelligence Analyst (UZ-876B2)',\n",
       "  ' 3.0  -  148 reviews',\n",
       "  'Job Details\\n\\nLevel\\nExperienced\\n\\nJob Location\\nHQ- Lehi - Lehi, UT\\n\\nPosition Type\\nFull Time\\n\\nTravel Percentage\\n0-25%\\n\\nJob Shift\\nDay\\n\\nCompany Overview\\n\\nPURPLE\\n\\nPurple is a digitally-native vertical brand with a mission to help people feel and live better through innovative comfort solutions. To us, comfort means more than great products, it means empowering every employee to feel comfortable being themselves. We believe your career at Purple will be a one-of-a-kind \"Career in Comfort\" because our workforce is one-of-a-kind. We are committed to a culture of collaboration where every voice is heard and understood. As an innovation company at our core, we believe a diversity workforce brings better insights, solutions and products and serves as the backbone to bettering our company. Join with us as we add to our team of exceptional individuals who will help us take over the world - one mattress at a time.\\n\\nPosition Summary\\n\\nDo you love data? Do you want to make a huge impact at an awesome...\\nREAD MORE'],\n",
       " ['Trimble',\n",
       "  'Business Intelligence Analyst (TO-EC0F3)',\n",
       "  ' 3.8  -  25 reviews',\n",
       "  'Title: Business Intelligence Analyst Location: Dayton, OH Department: IS - Information Services Trimble is an exciting, entrepreneurial company with a history of exceptional growth coupled with a disciplined and strategic focus on being the best. While GPS is at our core, we have grown beyond this technology to embrace other sophisticated positioning technologies and, in doing so, we are changing the way the world works. Our solutions are used in over 140 countries and we have incredibly diverse lines of business. Our employees represent this diversity and can be found in over 30 countries, working closely with their colleagues around the world. Due to our geographic, product and customer reach, there is plenty of room at Trimble for exceptional people to grow. Come position yourself with an innovative industry leader and position yourself for success. Job Summary: The Business Intelligence Analyst will be responsible as an implementation product owner within Trimble‚Äôs BI team. The...\\nREAD MORE'],\n",
       " ['Newsela',\n",
       "  'Business Intelligence Analyst, Customer Optimization',\n",
       "  ' 4.7  -  1,816 reviews',\n",
       "  'The Role\\n\\nThe Customer Optimization team is charged with leading efforts that will enable Newsela‚Äôs Customer (Sales & Customer Success) organization to scale. In this newly created role, you will help build the capabilities to measure the effectiveness and impact of the various initiatives the Optimization team pursues. Working closely with our Customer Optimization team, along with our centralized BI & Data Engineering teams, you‚Äôll build out our reporting and dashboard suite that will provide visibility into how completed projects impact our business performance.\\n\\nWhy You‚Äôll Love This Role\\n\\nYou will help build an optimization function that applies data to the operational aspects of our business. Your work will help us drive more business, more quickly by providing visibility to the various aspects of our operations from employee efficiency to data accuracy. Newsela is a data-hungry organization with a huge appetite for using data to help make decisions and drive the business; you‚Äôll...\\nREAD MORE'],\n",
       " ['Walker Edison Furniture Company LLC',\n",
       "  'Business Intelligence Analyst',\n",
       "  ' 4.7  -  1,816 reviews',\n",
       "  'Business Intelligence Analyst\\n\\nWalker Edison are seeking a Business Intelligence Analyst to join our team. This dynamic individual will serve as a central point for the interface of systems and data. The ideal candidate will be able to work independently and in a team environment, be adaptable to change, and be seeking a long-term opportunity with potential for growth in a fast-growing company.\\n\\nThe selected individual will be responsible for but not limited to the following:\\n‚Ä¢ Develop, publish, and maintain SQL queries\\n‚Ä¢ Act as the subject matter expert for all data and reporting\\n‚Ä¢ Ensure BI Software data reporting integrity\\n‚Ä¢ Assist in integration of additional outside data sources into BI Software\\n‚Ä¢ Setup and monitor meaningful reporting alerts and KPIs\\n‚Ä¢ Ad-hoc analysis and reporting for multiple stakeholders\\n\\nQualifications:\\n‚Ä¢ Bachelor‚Äôs degree in Business Management, Finance, Analytics, Statistics, Information Systems, or related field from an accredited university\\n‚Ä¢ 2+ years of...\\nREAD MORE'],\n",
       " ['Robert Half',\n",
       "  'Business Intelligence (BI) Analyst - Interim',\n",
       "  ' 4.0  -  344 reviews',\n",
       "  \"I am currently sourcing a Senior BI Developer - consultant for a remote role. If interested in learning more please reach out to Stephanie Luntz (615) 460-1212.\\n\\nHere you‚Äôll get to:\\n\\n‚Ä¢ Analyze business needs to produce business intelligence solutions to maximize efficiency of reporting and analysis\\n\\n‚Ä¢ Provide efficient BI solutions across multiple platforms and departments, standardizing and streamlining existing processes\\n\\n‚Ä¢ Create simple user interface solutions to complex processes\\n\\n‚Ä¢ Use various programming languages, including SQL and DAX queries, but primarily Visual Basic for Applications (VBA) in the Microsoft Office Suite to automate tasks and processes\\n\\n‚Ä¢ Work as the lead analyst/developer on the analytics team by managing multiple projects and deadlines\\n\\n‚Ä¢ Work effectively and efficiently in a fast-paced environment as part of a high-performing team\\n\\nFor this role:\\n\\n‚Ä¢ Bachelor's degree in computer science, finance, business, or a related field\\n\\n‚Ä¢ A minimum of 5+ years of...\\nREAD MORE\"],\n",
       " ['PerkSpot',\n",
       "  'Business Intelligence Analyst',\n",
       "  ' 4.5  -  16 reviews',\n",
       "  \"Work directly with America's best employers to bring joy to millions of employees!\\nNote: At PerkSpot, the safety and wellness of our employees is our top priority. To provide our employees with the support they may need to maintain their personal health and wellness, our office is open and available for those employees who are interested in using it. Our long term goal is for PerkSpot employees to return to our Chicago office, however working remotely will be an option until everyone is comfortable, and it is 100% safe for all to return.\\nHi, welcome to PerkSpot. We‚Äôre looking for a strategic and highly analytical Business Intelligence Analyst to join our growing team and help analyze data within PerkSpot‚Äôs platform, one used by employees working at some of the largest companies in America.\\nData analysis and reporting is some of the most important work we do here at PerkSpot. In this role, you will work with the rest of the Business Intelligence team to build and maintain reports that...\\nREAD MORE\"],\n",
       " ['GoodRx',\n",
       "  'Business Intelligence Analyst, Subscriptions',\n",
       "  ' 4.6  -  4,480 reviews',\n",
       "  \"At GoodRx, we believe that all Americans should have access to convenient and affordable healthcare. As a nation, we spend about $3.5 trillion annually on our healthcare, but too many Americans struggle to get the care they need, and prices just keep rising. Our marketplaces for prescription medicines and telehealth have helped Americans save $25 billion since 2011. GoodRx is a public company; we're based in Santa Monica with additional offices around the country. We're a low-key and tight-knit group that likes to find new ways to fix big problems. If you share our belief that you can do well by doing good, let's talk.\\nWe‚Äôre committed to growing and empowering a more inclusive community within our company and industry. That‚Äôs why we hire and cultivate diverse teams of the best and brightest from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel.\\nThe Business...\\nREAD MORE\"],\n",
       " ['EPAM Systems',\n",
       "  'Senior BI Analyst',\n",
       "  ' 4.1  -  75 reviews',\n",
       "  \"EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers' business, and strive for the highest standards of excellence. In today's new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you'll join a dedicated, diverse community that will help you discover your fullest potential.\\n\\nDescription\\nWe are looking for a talented Senior BI Analysts in the United States to extend our team of professionals who can help our customers achieving their digital transformation through data in variety of industries. We are passionate about delivering value from data and are searching for people with the same mindset to join our...\\nREAD MORE\"],\n",
       " ['Insight',\n",
       "  'Business Intelligence Analyst - Compensation Data (UZ-D3CBD)',\n",
       "  ' 4.1  -  75 reviews',\n",
       "  'Business Intelligence Analyst ‚Äì Business Intelligence Team UK ‚Äì Flexible on Location About Insight: We believe that by giving you the freedom to think big and empower you to reach your full potential, together we will achieve the best outcomes.\\n\\nLike the look of this opportunity? Make sure to apply fast, as a high volume of applications is expected. Scroll down to read the complete job description.\\n\\nAlong with excellent benefits and a compelling reward package, we offer the opportunity to work in a supportive environment with a high level of autonomy and creativity - there‚Äôs a reason our average employee tenure is over 6 years. We strive to display our three core values of Hunger, Heart and Harmony every day.\\n\\nThey represent and drive who we are here at Insight and by doing so we are doing amazing things.\\n\\nInsight started in a garage in 1988 and it is through harnessing our three core values that two brothers, Eric and Tim Crown, steered Insight to the Fortune 500 company it is...\\nREAD MORE'],\n",
       " ['Kivish Infotech Corp',\n",
       "  'Business Intelligence/BI Analyst',\n",
       "  ' 4.1  -  75 reviews',\n",
       "  'Kivish Infotech, Corp has some of the most sought-after Information Technology positions available. As a reputable company in the IT staffing industry, you can trust us to place you in the right position.\\n\\nPlease find the JD:\\n\\nSENIOR BI ANALYST || 6-8 months || 100% remote\\n\\nWhat you will be doing:\\n‚Ä¢ Deep understanding of Tableau development skills, business processes understanding, data analysis, communication and advanced SQL querying skills.\\n‚Ä¢ Ability to support the creation of single source of truth for Business Metrics leveraging agreed upon data definitions, build a common data foundation, and semantic layers to be consumed by business in Tableau\\n‚Ä¢ Establish standards and best practices for dashboards for the project and server administration automation.\\n‚Ä¢ Support the team with an understanding of business and data requirements, and iteratively design visualizations for customer presentation\\n‚Ä¢ Hands on experience and expertise with Tableau Online, data modeling/data analysis and...\\nREAD MORE'],\n",
       " ['Jabil Careers',\n",
       "  'Business Intelligence Analyst II',\n",
       "  ' 4.0  -  2 reviews',\n",
       "  'JOB SUMMARY\\n\\nThe Business Intelligence (BI) position is one of the key positions in the SAP Business Intelligence Team (BI) within the Global Corporate Information Technology.\\n\\nAs a member of Business Intelligence team implementing enterprise data warehousing, based upon SAP BW architecture, the BI Architect is responsible for evolution and production support of the data warehouse architecture and BI environment.\\n\\nESSENTIAL DUTIES AND RESPONSIBILITIES\\n‚Ä¢ Work collaboratively with the business community and ITS colleagues to establish a secure, effective, and efficient data warehouse architecture supporting multiple business subject areas.\\n‚Ä¢ Work directly with functional analysts and project teams as a lead or team member to develop new data warehouse functionality and architecture from business specifications by modeling, designing, developing, testing and implementing the appropriate back-end structures (including Infosources, ODSs, and Info Cubes) needed to meet business...\\nREAD MORE']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 4 excluded.\n",
      "Number 5 excluded.\n"
     ]
    }
   ],
   "source": [
    "# If first review is greater than a specific number\n",
    "\n",
    "new_list = []\n",
    "\n",
    "threshold = 3.5\n",
    "\n",
    "for a in range(0,x):\n",
    "    if total_list[a][2] == 'no reviews':\n",
    "        print('no reviews for number ' + str(a) + '.')\n",
    "    elif float(total_list[a][2].split(' ')[1]) > float(threshold):\n",
    "        new_list.append(total_list[a])\n",
    "    else:\n",
    "        print('Number ' + str(a + 1) + ' excluded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the lists into a string\n",
    "\n",
    "body_email = ' '\n",
    "\n",
    "for a in range(0,len(new_list)):\n",
    "    body_email += '\\r\\r\\n'\n",
    "    for b in range(0,4):\n",
    "        body_email += new_list[a][b] + '\\r\\r\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Sent!\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "sender_email = ''\n",
    "receiver_email = 'samuel2wright@gmail.com'\n",
    "\n",
    "gmail_password = ''\n",
    "\n",
    "\n",
    "msg = MIMEMultipart(\"alternative\")\n",
    "msg[\"Subject\"] = \"Jobs for \" + search_term +  \"\"\n",
    "msg[\"From\"] = sender_email\n",
    "msg[\"To\"] = receiver_email\n",
    "\n",
    "new = MIMEText(body_email, 'plain')\n",
    "\n",
    "msg.attach(new)\n",
    "\n",
    "body = msg.as_string()\n",
    "\n",
    "try:\n",
    "    \n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.ehlo()\n",
    "    server.login(sender_email, gmail_password)\n",
    "    server.sendmail(sender_email, receiver_email, body)\n",
    "    server.close()\n",
    "    print('Email Sent!')\n",
    "except:\n",
    "    print('Something went wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "#### üé©üé©üé©üé©üé©üé©üé©üé©üé©üé©\n",
    "\n",
    "Really im just going to tip my hat to web crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org/headers')\n",
    "pprint(r.json())\n",
    "\n",
    "User agents\n",
    "\n",
    "/robotx.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class BrickSetSpider(scrapy.Spider):\n",
    "    name = 'brick_spider'\n",
    "    start_urls = ['http://brickset.com/sets/year-2016']\n",
    "\n",
    "    def parse(self, response):\n",
    "        SET_SELECTOR = '.set'\n",
    "        for brickset in response.css(SET_SELECTOR):\n",
    "\n",
    "            NAME_SELECTOR = 'h1 ::text'\n",
    "            PIECES_SELECTOR = './/dl[dt/text() = \"Pieces\"]/dd/a/text()'\n",
    "            MINIFIGS_SELECTOR = './/dl[dt/text() = \"Minifigs\"]/dd[2]/a/text()'\n",
    "            IMAGE_SELECTOR = 'img ::attr(src)'\n",
    "            yield {\n",
    "                'name': brickset.css(NAME_SELECTOR).extract_first(),\n",
    "                'pieces': brickset.xpath(PIECES_SELECTOR).extract_first(),\n",
    "                'minifigs': brickset.xpath(MINIFIGS_SELECTOR).extract_first(),\n",
    "                'image': brickset.css(IMAGE_SELECTOR).extract_first(),\n",
    "            }\n",
    "\n",
    "        NEXT_PAGE_SELECTOR = '.next a ::attr(href)'\n",
    "        next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.hartleybrody.com/prevent-scrapers\n",
    "\n",
    "https://towardsdatascience.com/everything-you-need-to-know-about-web-scraping-6541b241f27e\n",
    "\n",
    "https://towardsdatascience.com/top-25-selenium-functions-that-will-make-you-pro-in-web-scraping-5c937e027244\n",
    "\n",
    "https://scrapy.org\n",
    "\n",
    "https://stackabuse.com/how-to-send-emails-with-gmail-using-python\n",
    "\n",
    "https://realpython.com/python-requests/#request-headers\n",
    "\n",
    "https://www.youtube.com/channel/UC4JX40jDee_tINbkjycV4Sg\n",
    "\n",
    "https://www.w3schools.com/tags/default.asp\n",
    "\n",
    "https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "https://www.restapitutorial.com/lessons/httpmethods.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
