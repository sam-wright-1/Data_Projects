{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping and Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests\n",
    "\n",
    "pip install requests\n",
    "\n",
    "## ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è\n",
    "\n",
    " #### Base of Web communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get() # Read\n",
    "requests.post() # Create\n",
    "requests.patch() # Update (modify)\n",
    "requests.put() # Update (replace)\n",
    "requests.delete() # Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from base64 import b64encode \n",
    "from pprint import pprint \n",
    "import sys\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from six.moves import urllib\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creds\n",
    "BASE_API_URL = \"\"\n",
    "EMAIL = \"\"\n",
    "API_KEY = \"\"\n",
    "\n",
    "#The cell creates a connection with our database and pulls the last order that was put into the database.\n",
    "\n",
    "#Make sure the input the login creds\n",
    "params = urllib.parse.quote_plus('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                                 'SERVER=;'\n",
    "                                 'DATABASE=;'\n",
    "                                 'Trusted_Connection=;'\n",
    "                                 'UID=;'\n",
    "                                 'PWD='\n",
    "                                )\n",
    "\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "\n",
    "engine.connect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the authentication step\n",
    "basic_auth = b64encode(f'{GORGIAS_EMAIL}:{GORGIAS_API_KEY}'.encode('utf-8')).decode('utf-8')\n",
    "newheaders = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Basic {basic_auth}'\n",
    "}\n",
    "\n",
    "#this grabs the total pages needed\n",
    "Pages = requests.get(f'{GORGIAS_BASE_API_URL}/tickets/?per_page=100',\n",
    "            headers= newheaders)\n",
    "Request_Pages = pd.json_normalize(Pages.json())\n",
    "Num_Pages = Request_Pages['meta.nb_pages']\n",
    "num = int(Num_Pages)\n",
    "# num can be used to loop through to get all of the results\n",
    "\n",
    "Request_Answer = requests.get(f'{GORGIAS_BASE_API_URL}/tickets/?view_id=12734&per_page=100',\n",
    "          headers= newheaders)\n",
    "\n",
    "Ticket_Dataframe = pd.json_normalize(Request_Answer.json())\n",
    "Ticket_Data_Dataframe = Ticket_Dataframe['data']\n",
    "\n",
    "AllTickets = pd.json_normalize(Ticket_Data_Dataframe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTickets_Database.to_sql(name='',con=engine, index=False, if_exists='replace', method = 'multi',chunksize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "pip install bs4\n",
    "\n",
    "## üëçüëçüëçüëçüëçüëçüëçüëç\n",
    "\n",
    "\n",
    "###### Find data by HTML Tags\n",
    "\n",
    "###### Ways to parse through data\n",
    " - html: standard\n",
    " - lxml: goes fast\n",
    " - html5lib: does it like a web page would but slow\n",
    " - xml: supports xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag is the BeatuifulSoup object\n",
    "\n",
    "print(page.status_code) - lets you know if the request was successful\n",
    "print(page.headers) \n",
    "\n",
    "soup.get_text() # gets all of the text from a page\n",
    "soup.find('h2', class_='title')\n",
    "soup.find('div', class_='company')\n",
    "soup.findall(\"a\", attrs={'class':''}) # finding all links (a-hrefs) in a certain class\n",
    "\n",
    "tag.contents # gets the values inside of the html tags after you find the one you are looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: grabbing stock prices from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stocks you want to check\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "stock_list = {'GOOG', 'TSLA', 'ADBE', 'RACE', 'DOMO','NKE', 'DAL', 'CRM', 'PS', 'ADSK', 'AAPL', 'AMZN', 'BABA', 'IBM', 'LOGI', 'OKTA'}\n",
    "\n",
    "for urls in stock_list:\n",
    "\n",
    "    URL = 'https://finance.yahoo.com/quote/'+ urls + '?p= '+ urls +'&.tsrc=fin-srch'\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tag = soup.find('span', class_='Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)') # Creates a bs4 element tag.\n",
    "\n",
    "    value = tag.contents[0] # list\n",
    "    print(urls,'-',value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Price and Description of Amazon Product\n",
    "\n",
    "#### Not my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib\n",
    "from pprint import pprint\n",
    "\n",
    "headers = {\n",
    "    \"User-agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0'}\n",
    "\n",
    "URL = 'https://www.amazon.de/dp/B07XVWXW1Q/ref=sr_1_10?keywords=laptop&qid=1581888312&sr=8-10'\n",
    "def amazon_de():\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"productTitle\").get_text()\n",
    "    price = soup.find(id=\"priceblock_ourprice\").get_text()\n",
    "    sep = ','\n",
    "    con_price = price.split(sep, 1)[0]\n",
    "    converted_price = int(con_price.replace('.', ''))\n",
    "\n",
    "    # price\n",
    "    print(title.strip())\n",
    "    print(converted_price)\n",
    "    \n",
    "    return converted_price, title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "price, title = amazon_de()\n",
    "\n",
    "# Could do\n",
    "if price < 50:\n",
    "    email = True\n",
    "else:\n",
    "    email = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if email = True:\n",
    "\n",
    "    email = \n",
    "    password = \n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login(email, password)\n",
    "    subject = 'Price fell down!'\n",
    "    body = 'Check the link:\\\n",
    "    https://www.amazon.de/gp/product/B0756CYWWD/ref=as_li_tl?ie=UTF8&tag=idk01e-21&camp=1638&creative=6742&linkCode=as2&creativeASIN=B0756CYWWD&linkId=18730d371b945bad11e9ea58ab9d8b32'\n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    server.sendmail(\n",
    "    'Price check',\n",
    "    'email-address',\n",
    "    msg\n",
    "    )\n",
    "    print('Hey Email has been sent!')\n",
    "    server.quit()\n",
    "else:\n",
    "    print('not below 50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Amazon Example\n",
    "\n",
    "#### Not my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "     \n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    " \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.string\n",
    " \n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    " \n",
    "        # # Printing types of values for efficient understanding\n",
    "        # print(type(title))\n",
    "        # print(type(title_value))\n",
    "        # print(type(title_string))\n",
    "        # print()\n",
    " \n",
    "    except AttributeError:\n",
    "        title_string = \"\"   \n",
    " \n",
    "    return title_string\n",
    " \n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    " \n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    " \n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    " \n",
    "        except:     \n",
    "            price = \"\"  \n",
    " \n",
    "    return price\n",
    " \n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    " \n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "         \n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\" \n",
    " \n",
    "    return rating\n",
    " \n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "        review_count = \"\"   \n",
    " \n",
    "    return review_count\n",
    " \n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\n",
    " \n",
    "    return available    \n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # Headers for request\n",
    "    HEADERS = ({'User-Agent':\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "                'Accept-Language': 'en-US'})\n",
    " \n",
    "    # The webpage URL\n",
    "    URL = \"https://www.amazon.com/s?k=playstation+4&ref=nb_sb_noss_2\"\n",
    "     \n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    " \n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    " \n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    " \n",
    "    # Store the links\n",
    "    links_list = []\n",
    " \n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    " \n",
    " \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    " \n",
    "        new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    " \n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "         \n",
    "        # Function calls to display all necessary product information\n",
    "        print(\"Product Title =\", get_title(new_soup))\n",
    "        print(\"Product Price =\", get_price(new_soup))\n",
    "        print(\"Product Rating =\", get_rating(new_soup))\n",
    "        print(\"Number of Product Reviews =\", get_review_count(new_soup))\n",
    "        print(\"Availability =\", get_availability(new_soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium \n",
    "\n",
    "## üî•üî•üî•üî•üî•üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get() # Opens up the page\n",
    "title # prints webpage title\n",
    "quit() # Quit browser \n",
    "close() # Close tab\n",
    "click()\n",
    "send_keys()\n",
    "find_element()\n",
    "back()\n",
    "forward()\n",
    "clear()\n",
    "\n",
    "https://selenium-python.readthedocs.io/locating-elements.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    jobs = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"main\"))\n",
    "    )\n",
    "except:\n",
    "    print('not working')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Jobs off Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XPath is a query language for selecting nodes from an XML document. You can use it with HTML and its really handy.\n",
    "#  Basically it just shows the path to where the element is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Webdriver performs the actions and syncs up with the page\n",
    "from selenium import webdriver # Have the ability to open up a page and access it\n",
    "from selenium.webdriver.common.keys import Keys # access to enter key, escape key, etc.\n",
    "from selenium.webdriver.common.by import By # the thing to search \n",
    "from selenium.webdriver.support.ui import WebDriverWait # Wait until something renders\n",
    "from selenium.webdriver.support import expected_conditions as EC # wait until a condition is met\n",
    "from selenium.webdriver.common.action_chains import ActionChains # perform more advanced tasks (double clicks, drag and drop, etc.)\n",
    "\n",
    "import time\n",
    "\n",
    "# Set up Selenium\n",
    "path_to_chromedriver = 'C:\\Program Files (x86)\\chromedriver.exe' # this part needs the chrome driver installed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver) # This is where the driver lives\n",
    "\n",
    "\n",
    "# ONE WAY TO DO IT\n",
    "search_term = 'Data Science Intern'\n",
    "search = search_term.replace(' ', '+')\n",
    "\n",
    "URL_last_three = 'https://www.google.com/search?q='+ search +'&rlz=1C1CHBF_enUS908US908&oq=goiogle+jovs&aqs=chrome..69i57j0i13i457j0i13l8.1319j0j7&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiju-3NoPXuAhVQpZ4KHTjRDgUQutcGKAF6BAgdEAk&sxsrf=ALeKk01FkBWPIp-F8Rs4nOmTOe-XTXosyw:1613713865670#fpstate=tldetail&htivrt=jobs&htichips=date_posted:3days&htischips=date_posted;3days&htidocid=Ob4dWFbxVOjFasLVAAAAAA%3D%3D'\n",
    "URL_all = 'https://www.google.com/search?q='+ search +'&rlz=1C1CHBF_enUS908US908&oq=goiogle+jovs&aqs=chrome..69i57j0i13i457j0i13l8.1319j0j7&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiju-3NoPXuAhVQpZ4KHTjRDgUQutcGKAF6BAgdEAk&sxsrf=ALeKk01FkBWPIp-F8Rs4nOmTOe-XTXosyw:1613713865670#fpstate=tldetail&htivrt=jobs&htidocid=TDq3YZR6oHJft8dBAAAAAA%3D%3D'\n",
    "\n",
    "# Use selenium\n",
    "\n",
    "browser.get(URL_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"7c9559f2-ed2a-41d2-ae8e-12a5e27db789\")>\n",
      " 4.1  -  379 reviews\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"5c7ae6a6-71fd-407f-a744-dc59a0d45a74\")>\n",
      " 4.3  -  20 reviews\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"ac8c9458-0071-40db-8e86-a078b70b3add\")>\n",
      " 4.5  -  1,240 reviews\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"7c9559f2-ed2a-41d2-ae8e-12a5e27db789\")>\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"5c7ae6a6-71fd-407f-a744-dc59a0d45a74\")>\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"1d2500194e7d88dd2b9aaac3d2048bd8\", element=\"ac8c9458-0071-40db-8e86-a078b70b3add\")>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver # Have the ability to open up a page and access it\n",
    "from selenium.webdriver.common.keys import Keys # access to enter key, escape key, etc.\n",
    "from selenium.webdriver.common.by import By # the thing to search \n",
    "from selenium.webdriver.support.ui import WebDriverWait # Wait until something renders\n",
    "from selenium.webdriver.support import expected_conditions as EC # wait until a condition is met\n",
    "from selenium.webdriver.common.action_chains import ActionChains # perform more advanced tasks (double clicks, drag and drop, etc.)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Search jobs\n",
    "search_term = 'Data Engineer'\n",
    "\n",
    "# How many job postings do you want?\n",
    "job_postings = 3\n",
    "\n",
    "# Set up Selenium\n",
    "path_to_chromedriver = 'C:\\Program Files (x86)\\chromedriver.exe' # this part needs the chrome driver installed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver) # This is where the driver lives\n",
    "\n",
    "\n",
    "# Use selenium\n",
    "browser.get('https://www.google.com/search?q=job&rlz=1C1CHBF_enUS908US908&oq=google+jobs&aqs=chrome.0.69i59j0l2j0i433l2j69i60l3.1965j0j4&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwiM-Liuiv_uAhU5GDQIHZFoCvoQutcGKAF6BAgdEAk&sxsrf=ALeKk02jKAPaNxuq0NajGP0eUDp3I8To1g:1614051491600#fpstate=tldetail&htivrt=jobs&htidocid=6ro2iZ7gqMVEcaUzAAAAAA%3D%3D') # Opens up the page\n",
    "\n",
    "\n",
    "# This part gets us the jobs from the search term and the ones that are the newest\n",
    "search = browser.find_element_by_id(\"hs-qsb\")\n",
    "search.clear()\n",
    "search.send_keys(search_term)\n",
    "search.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "search = browser.find_element_by_xpath('//*[@id=\"choice_box_root\"]/div[1]/div[1]/span[3]')\n",
    "search.click()\n",
    "\n",
    "time.sleep(2) \n",
    "\n",
    "search = browser.find_element_by_xpath('//div[@data-name = \"3days\"]')\n",
    "search.click()\n",
    "\n",
    "time.sleep(2) \n",
    "\n",
    "# userNameElement= WebDriverWait(browser, 10).until(\n",
    "# EC.presence_of_element_located((By.XPATH, '//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[1]/div/div[1]/div[2]'))\n",
    "# )\n",
    "# userNameElement.click()\n",
    "\n",
    "\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[1]/div/div[1]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[2]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[3]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[4]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[5]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[6]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[7]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[8]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[9]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[1]/div/ul/li[10]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[2]/div/ul/li[1]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[2]/div/ul/li[8]/div/div[2]/div[2]\n",
    "#//*[@id=\"VoQFxe\"]/div[3]/div/ul/li[1]/div/div[2]/div[2]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Better way to do it\n",
    "\n",
    "\n",
    "x = 30 # Amount of job postings I want \n",
    "y = int(x / 10) # pull 10 accourding to html\n",
    "z = x % 10 # remainder\n",
    "\n",
    "# This is the way I scroll\n",
    "for i in range(1,y+1):\n",
    "    scroll = ActionChains(browser)\n",
    "    scroll.move_to_element(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(i) + ']/div/ul/li[9]/div/div[2]/div[2]'))\n",
    "    scroll.click()\n",
    "    scroll.pause(2)\n",
    "    scroll.perform()\n",
    "    \n",
    "# Finds loads\n",
    "better = browser.find_elements_by_class_name('gws-plugins-horizon-jobs__tl-lif')\n",
    "for elements in better:\n",
    "    actions = ActionChains(browser)\n",
    "    actions.move_to_element(elements)\n",
    "    actions.click()\n",
    "    actions.pause(1)\n",
    "    actions.perform()\n",
    "\n",
    "\"\"\"    \n",
    "    \n",
    "x = job_postings # Amount of job postings I want \n",
    "\n",
    "y = int(x / 10) # pull 10 accourding to html\n",
    "z = x % 10 # remainder\n",
    "\n",
    "# This is the way I scroll to populate further pages\n",
    "for i in range(1,y+1):\n",
    "    scroll = ActionChains(browser)\n",
    "    scroll.move_to_element(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(i) + ']/div/ul/li[9]/div/div[2]/div[2]'))\n",
    "    scroll.click()\n",
    "    scroll.pause(2)\n",
    "    scroll.perform()\n",
    "\n",
    "# All of the elements that we need to access\n",
    "tree_items = []\n",
    "\n",
    "for m in range(1,y + 1):\n",
    "    for n in range(1,11):\n",
    "        if m == 1 and n == 1:\n",
    "            l = 1\n",
    "        else: \n",
    "            l = 2\n",
    "        tree_items.append(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(m) + ']/div/ul/li[' + str(n) + ']/div/div[' + str(l) + ']/div[2]/div/div'))\n",
    "    \n",
    "remainder_items = []\n",
    "\n",
    "for k in range(1,z+1):\n",
    "    if y == 0 and k == 1:\n",
    "        l = 1\n",
    "    else: \n",
    "        l = 2\n",
    "    remainder_items.append(browser.find_element_by_xpath('//*[@id=\"VoQFxe\"]/div[' + str(y+1) + ']/div/ul/li[' + str(k) + ']/div/div[' + str(l) + ']/div[2]/div/div'))\n",
    "\n",
    "\n",
    "    \n",
    "total_list = []\n",
    "job_list = []\n",
    "\n",
    "for elements in tree_items:\n",
    "        first = ActionChains(browser)\n",
    "        first.move_to_element(elements)\n",
    "        first.click()\n",
    "        first.pause(2)\n",
    "        first.perform()\n",
    "        \n",
    "        job_list = []\n",
    "        \n",
    "        try:\n",
    "            company_name = browser.find_elements_by_xpath('//*[@id=\"gws-plugins-horizon-jobs__job_details_page\"]/div/div[1]/div/div[2]/div[2]/div[1]')\n",
    "            for i in company_name:\n",
    "                if i.is_displayed():\n",
    "                    company_name_final = i.text\n",
    "        except:\n",
    "            company_name_final = 'no company name'\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            title = browser.find_elements_by_class_name('KLsYvd')\n",
    "            for i in title:\n",
    "                if i.is_displayed():\n",
    "                    title_final = i.text\n",
    "                \n",
    "        except:\n",
    "            title_final = 'no title'\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # review = browser.find_element_by_xpath('//*[@id=\"fc_fDw1YPeFLP-V0PEPxZqi0A06\"]/div/a[1]/div/div[2]').get_attribute('textContent')\n",
    "            description = browser.find_elements_by_class_name('YgLbBe')\n",
    "            for i in description:\n",
    "                if i.is_displayed():\n",
    "                    description_final = i.text\n",
    "                \n",
    "        except:\n",
    "            description_final = 'no description'\n",
    "                \n",
    "                        \n",
    "        try:\n",
    "            \n",
    "            reviews = browser.find_elements_by_class_name('o1h2dc')\n",
    "            for i in reviews:\n",
    "                if i.is_displayed():\n",
    "                    reviews_final = i.text\n",
    "\n",
    "        except:\n",
    "            \n",
    "            reviews_final = 'no reviews'\n",
    "            \n",
    "        \n",
    "        job_list.append(company_name_final)\n",
    "        \n",
    "        job_list.append(title_final)\n",
    "        \n",
    "        job_list.append(reviews_final)\n",
    "        \n",
    "        job_list.append(description_final)\n",
    "        \n",
    "        # Append to the total list\n",
    "        total_list.append(job_list)\n",
    "\n",
    "        \n",
    "for elements in remainder_items:\n",
    "        second = ActionChains(browser)\n",
    "        second.move_to_element(elements)\n",
    "        second.click()\n",
    "        second.pause(2)\n",
    "        second.perform()\n",
    "        \n",
    "        job_list = []\n",
    "        \n",
    "        try:\n",
    "            company_name = browser.find_elements_by_xpath('//*[@id=\"gws-plugins-horizon-jobs__job_details_page\"]/div/div[1]/div/div[2]/div[2]/div[1]')\n",
    "            for i in company_name:\n",
    "                if i.is_displayed():\n",
    "                    company_name_final = i.text\n",
    "        except:\n",
    "            company_name_final = 'no company name'\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            title = browser.find_elements_by_class_name('KLsYvd')\n",
    "            for i in title:\n",
    "                if i.is_displayed():\n",
    "                    title_final = i.text\n",
    "                \n",
    "        except:\n",
    "            title_final = 'no title'\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # review = browser.find_element_by_xpath('//*[@id=\"fc_fDw1YPeFLP-V0PEPxZqi0A06\"]/div/a[1]/div/div[2]').get_attribute('textContent')\n",
    "            description = browser.find_elements_by_class_name('YgLbBe')\n",
    "            for i in description:\n",
    "                if i.is_displayed():\n",
    "                    description_final = i.text\n",
    "                \n",
    "        except:\n",
    "            description_final = 'no description'\n",
    "                \n",
    "                        \n",
    "        try:\n",
    "            \n",
    "            reviews = browser.find_elements_by_class_name('o1h2dc')\n",
    "            for i in reviews:\n",
    "                if i.is_displayed():\n",
    "                    reviews_final = i.text\n",
    "\n",
    "        except:\n",
    "            \n",
    "            reviews_final = 'no reviews'\n",
    "            \n",
    "        \n",
    "        job_list.append(company_name_final)\n",
    "        \n",
    "        job_list.append(title_final)\n",
    "        \n",
    "        job_list.append(reviews_final)\n",
    "        \n",
    "        job_list.append(description_final)\n",
    "        \n",
    "        # Append to the total list\n",
    "        total_list.append(job_list)\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Aktify',\n",
       "  'Senior Data Engineer / Machine Learning',\n",
       "  ' 2.9  -  10,788 reviews',\n",
       "  'Job Description\\n\\nAktify, Inc. was founded in 2017 and is based in Lehi, Utah. Although we are an early-stage R&D company, we already deliver 10X ROI for our clients (including several large national brands). At Aktify, we are personalizing interactions between brands and consumers at scale with machine learning. Aktify uses NLP / NLU and deep learning to deliver natural, human-like conversations via sms texting, instant messaging, email, and web pages. We are creating a world class AI platform to master conversations, delight consumers and drive improved sales, marketing, and service results for our corporate clients.\\n\\nAbout the Senior Data Engineer/Machine Learning Engineer position:\\n\\nThe Senior Machine Learning Engineer role is focused on building out and scaling up our K8s microservices AI platform based on reactive engineering principles. You will be wrapping our state of the art deep learning models in robust, reactive services on our event driven lambda architecture. You will...\\nREAD MORE'],\n",
       " ['Pluralsight',\n",
       "  'Data Engineer',\n",
       "  ' 4.5  -  1,240 reviews',\n",
       "  'This position is available for remote employment in these areas:\\nDraper UT, Remote - California (Bay Area), Remote - Illinois (Chicago), Remote - New Jersey (NYC Metro Area), Remote - New York, Remote - New York (New York City), Remote - Washington (Seattle)Job Description:\\nOur Data Engineering & Operations team is a force multiplier for data practitioners at Pluralsight. We provide tooling and data sets to make Pluralsight a data-driven organization. Our work includes: building pipelines which curate and land data, deploying data science models, and maintaining data infrastructure. You‚Äôll have the opportunity to work with data tools, like Python and Spark, as well as web analytics and streaming data from our data platform.\\n\\nWho you‚Äôre committed to being:\\n‚Ä¢ You utilize a multidisciplinary approach to providing solutions for the business, combining technical, analytical, and domain knowledge.\\n‚Ä¢ You have strong development skills, experience transforming and profiling data\\n‚Ä¢ You...\\nREAD MORE'],\n",
       " ['Vivo Inc.',\n",
       "  'Principal Data Engineer',\n",
       "  ' 4.5  -  1,240 reviews',\n",
       "  'Our fast-growing client is in need of a Principal Data Engineer for their Utah location. Responsibilities Architecting, building, and maintaining modern, scalable data architectures in the cloud preferably AWS Design develop frameworks for increasing the overall efficiency of bringing data into the data lake, processing and delivery of data Encode best practices into reusable tools that can be shared across the team Automate the deployment of changes to production to improve data reliability quality Work closely with upstream and downstream stakeholders to collect business requirements and develop data models to satisfy those requirements Design and deliver event-driven solutions using a streaming platform such as Kafka Mentor data engineers and other stakeholders on the best practices of managing a large-scale production data platform and developing a data-driven operations culture within the data engineering team Evaluate new technologies and solutions for inclusion into the data...\\nREAD MORE']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If first review is greater than a specific number\n",
    "\n",
    "new_list = []\n",
    "\n",
    "threshold = 2.5\n",
    "\n",
    "for a in range(0,x):\n",
    "    if total_list[a][2] == 'no reviews':\n",
    "        print('no reviews for number ' + str(a) + '.')\n",
    "    elif float(total_list[a][2].split(' ')[1]) > float(threshold):\n",
    "        new_list.append(total_list[a])\n",
    "    else:\n",
    "        print('Number ' + str(a) + ' excluded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the lists into a string\n",
    "\n",
    "body_email = ' '\n",
    "\n",
    "for a in range(0,len(new_list)):\n",
    "    body_email += '\\r\\r\\n'\n",
    "    for b in range(0,4):\n",
    "        body_email += new_list[a][b] + '\\r\\r\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "sender_email = 'samthesoccerman360@gmail.com'\n",
    "receiver_email = 'samuel2wright@gmail.com'\n",
    "\n",
    "gmail_password = ''\n",
    "\n",
    "\n",
    "msg = MIMEMultipart(\"alternative\")\n",
    "msg[\"Subject\"] = \"Jobs for \" + search_term +  \"\"\n",
    "msg[\"From\"] = sender_email\n",
    "msg[\"To\"] = receiver_email\n",
    "\n",
    "new = MIMEText(body_email, 'plain')\n",
    "\n",
    "msg.attach(new)\n",
    "\n",
    "body = msg.as_string()\n",
    "\n",
    "try:\n",
    "    \n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.ehlo()\n",
    "    server.login(sender_email, gmail_password)\n",
    "    server.sendmail(sender_email, receiver_email, body)\n",
    "    server.close()\n",
    "    print('Email Sent!')\n",
    "except:\n",
    "    print('Something went wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "#### üé©üé©üé©üé©üé©üé©üé©üé©üé©üé©\n",
    "\n",
    "Really im just going to tip my hat to web crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org/headers')\n",
    "pprint(r.json())\n",
    "\n",
    "User agents\n",
    "\n",
    "/robotx.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class BrickSetSpider(scrapy.Spider):\n",
    "    name = 'brick_spider'\n",
    "    start_urls = ['http://brickset.com/sets/year-2016']\n",
    "\n",
    "    def parse(self, response):\n",
    "        SET_SELECTOR = '.set'\n",
    "        for brickset in response.css(SET_SELECTOR):\n",
    "\n",
    "            NAME_SELECTOR = 'h1 ::text'\n",
    "            PIECES_SELECTOR = './/dl[dt/text() = \"Pieces\"]/dd/a/text()'\n",
    "            MINIFIGS_SELECTOR = './/dl[dt/text() = \"Minifigs\"]/dd[2]/a/text()'\n",
    "            IMAGE_SELECTOR = 'img ::attr(src)'\n",
    "            yield {\n",
    "                'name': brickset.css(NAME_SELECTOR).extract_first(),\n",
    "                'pieces': brickset.xpath(PIECES_SELECTOR).extract_first(),\n",
    "                'minifigs': brickset.xpath(MINIFIGS_SELECTOR).extract_first(),\n",
    "                'image': brickset.css(IMAGE_SELECTOR).extract_first(),\n",
    "            }\n",
    "\n",
    "        NEXT_PAGE_SELECTOR = '.next a ::attr(href)'\n",
    "        next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.hartleybrody.com/prevent-scrapers\n",
    "\n",
    "https://towardsdatascience.com/everything-you-need-to-know-about-web-scraping-6541b241f27e\n",
    "\n",
    "https://towardsdatascience.com/top-25-selenium-functions-that-will-make-you-pro-in-web-scraping-5c937e027244\n",
    "\n",
    "https://scrapy.org\n",
    "\n",
    "https://stackabuse.com/how-to-send-emails-with-gmail-using-python\n",
    "\n",
    "https://realpython.com/python-requests/#request-headers\n",
    "\n",
    "https://www.youtube.com/channel/UC4JX40jDee_tINbkjycV4Sg\n",
    "\n",
    "https://www.w3schools.com/tags/default.asp\n",
    "\n",
    "https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "https://www.restapitutorial.com/lessons/httpmethods.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
